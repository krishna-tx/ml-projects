# nn-from-scratch

Above are some of the projects I have done this summer. I am still experimenting with the Feedforward Neural Network and Vanilla RNN from scratch using numpy and I am in the process of learning about different features such as optimization like RMSProp or Adam. Until now, all of my projects (except the ones I did through PyTorch) are using simple Gradient Descent.

## Here are some of the resources I used to gain knowledge in this field.

1. Professor Andrew Ng's [ML Class](ml-class.org)

