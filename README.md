# nn-from-scratch

Above are some of the projects I have done this summer. I am still experimenting with the Feedforward Neural Network and Vanilla RNN from scratch using numpy and I am in the process of learning about different features such as optimization like RMSProp or Adam. Until now, all of my projects (except the ones I did through PyTorch) are using simple Gradient Descent.

## Resources I used to gain knowledge in this field.

1. Professor Andrew Ng's [ML Class](https://www.coursera.org/learn/machine-learning)
2. Stanford Computer Vision [Youtube lectures](https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)
3. PadhAI [Intro to DataScience](https://padhai.onefourthlabs.in/courses/data-science)
4. PadhAI [Deep Learning](https://www.guvi.in/deep-learning)
5. University of Michigan Computer Vision [Lecture Recordings and Assignments](https://madewithml.com/projects/1575/deep-learning-for-computer-vision/)
