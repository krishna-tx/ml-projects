{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mDfhmj08R7H"
   },
   "source": [
    "# Introduction\n",
    "### In this notebook, I will be showing how to use a Convolutional Neural Network to create a model that should be able to identify digits from 0 to 9 from an image. The dataset is the MNist dataset which can be directly accessed through PyTorch. The model architecture that will be used is the famous Lenet-5 developed by Yann LeCun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moCeYeo68R7I"
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "To get started, we'll need to import some libraries from pytorch. In addition, we will also use matplotlib.pyplot to see our image data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1OnC-F-8R7J"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jO2hVks8R7N"
   },
   "source": [
    "We will import requests to get an image from the internet to test our model. We also need the *Image* module from the *PIL* library because that is what our testing image will be, an image. We also need to import the *copy* library in order to copy our model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUI6pUvt8R7N"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpgXYpNb8R7R"
   },
   "source": [
    "In our last import, we import other useful libraries such as sklearn only to see the confusion matrix of our model, pandas to see the confusion matrix as a table of values, and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViAS-PxX8R7S"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DDjYeNH8R7V"
   },
   "source": [
    "Set the batch size to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtHthHaB8R7W"
   },
   "outputs": [],
   "source": [
    "numb_batch = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diQU0g5a8R7Z"
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCiyiUWi8R7a"
   },
   "source": [
    "We need to transform the image into a tensor that can be used, so we do *torchvision.transforms.ToTensor()*.\n",
    "\n",
    "We get the training data from the Mnist library and set *download* to True. Then we need to transfrom the images.\n",
    "The same can be done for the validation data except that that *train* is False. \n",
    "\n",
    "We also need the dataloaders for each dataset and set the batch size to the wanted number, 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrkXPxb78R7b",
    "outputId": "e543f55f-ade9-4120-ffb3-230bbed5185c"
   },
   "outputs": [],
   "source": [
    "T = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "train_data = torchvision.datasets.MNIST('mnist_data', train=True, download=True, transform=T)\n",
    "val_data = torchvision.datasets.MNIST('mnist_data', train=False, download=True, transform=T)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size = numb_batch)\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size = numb_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pws6gYLx8R7r"
   },
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rgoPNFk8R7s"
   },
   "source": [
    "For the Mnist dataset, we will be using the LeNet 5 architecture as shown below:\n",
    "\n",
    "    ![title](Lenet-5.png)\n",
    "Image from https://www.researchgate.net/profile/Sheraz_Khan8/publication/321586653/figure/fig4/AS:568546847014912@1512563539828/The-LeNet-5-Architecture-a-convolutional-neural-network.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgLfjMl78R7t"
   },
   "outputs": [],
   "source": [
    "def create_lenet():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 6, 5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(2, stride=2),\n",
    "        nn.Conv2d(6, 16, 5, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(400, 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, 10)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iL5y5_5H8R7w"
   },
   "source": [
    "## Validating the Model\n",
    "\n",
    "This validation will use the validation set of handwritten digits and compute how many images are predicted right out of the total number of images. This is just a simple loop through every image in the validation dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tv2kj26h8R7x"
   },
   "outputs": [],
   "source": [
    "def validate(model, data):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(data):\n",
    "        images = images.cuda()\n",
    "        x = model(images)\n",
    "        value, pred = torch.max(x,1)\n",
    "        pred = pred.data.cpu()\n",
    "        total += x.size(0)\n",
    "        correct += torch.sum(pred == labels)\n",
    "    return correct*100./total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P7DUaLg8R70"
   },
   "source": [
    "## Training Function\n",
    "\n",
    "For training, we will set the default value to 3 epochs, the learning rate to 0.001, and the device to the cpu of the computer. These can be changed by using parameters when calling the *train* function.\n",
    "\n",
    "We will use Cross Entropy Loss as our loss function of choice and the Adam optimizer.\n",
    "\n",
    "An array of accuracies of the validation set is also being kept to plot the graph after the training process is finished for us to see how the model did on the validation.\n",
    "\n",
    "We only want the best model, so we have to keep a max_accuracy variable and keep updating it if a new accuracy is higher than maximum. Then we just copy the best model into *best_model* and then return *best_model* at the end of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhq8TZrt8R70"
   },
   "outputs": [],
   "source": [
    "def train(numb_epoch=3, lr=1e-2, device=\"cpu\"):\n",
    "    accuracies = []\n",
    "    cnn = create_lenet().to(device)\n",
    "    cec = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr=lr)\n",
    "    max_accuracy = 0\n",
    "    for epoch in range(numb_epoch):\n",
    "        for i, (images, labels) in enumerate(train_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = cnn(images)\n",
    "            loss = cec(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        accuracy = float(validate(cnn, val_dl))\n",
    "        accuracies.append(accuracy)\n",
    "        if accuracy > max_accuracy:\n",
    "            best_model = copy.deepcopy(cnn)\n",
    "            max_accuracy = accuracy\n",
    "            print(\"Saving Best Model with Accuracy: \", accuracy)\n",
    "        print('Epoch:', epoch+1, \"Accuracy :\", accuracy, '%')\n",
    "    plt.plot(accuracies)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vOHWqdsE8R74"
   },
   "source": [
    "## GPU Availability\n",
    "\n",
    "Now we check if a GPU is available. If so, then we can use it. If not, then we must resort to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0N6W0p98R74"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No Cuda Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIVrRTAY8R77",
    "outputId": "deda018c-dad8-413f-f95c-327a59a03fa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9xtlOiJ8R7_"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we will call the training function to actually train the model. Since the function will return the best model, we store it in the name *lenet*. I chose to call the function with 30 epochs but you can try different values and see what works best. Since I have a GPU available to use, I need to set the parameter to the GPU. If you do not have a GPU available, you can leave out the *device=device* because by default the function will use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckDdgUh38R7_"
   },
   "outputs": [],
   "source": [
    "device = torch.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJyAIO678R8D",
    "outputId": "268886b8-edf8-49ec-ad1e-04917543f8e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model with Accuracy:  97.61000061035156\n",
      "Epoch: 1 Accuracy : 97.61000061035156 %\n",
      "Saving Best Model with Accuracy:  97.9000015258789\n",
      "Epoch: 2 Accuracy : 97.9000015258789 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b7e4d5e3307b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlenet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-c01087da2544>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(numb_epoch, lr, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\ml\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\ml\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lenet = train(50, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdWTyLKL8R8I"
   },
   "source": [
    "## Saving the model\n",
    "\n",
    "We can save the model using *torch.save()* and give a path and name for the model\n",
    "\n",
    "If you want to save it in the same place you are using the notebook, you can just give the model name as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6QG2uQEE8R8I"
   },
   "outputs": [],
   "source": [
    "torch.save(lenet.state_dict(), \"lenet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKDnR9mm8R8L"
   },
   "source": [
    "## Optional: Loading the saved model \n",
    "\n",
    "If you saved the model, and don't want to train again, you can just load the model from the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtykXpGw8R8L",
    "outputId": "2fb19681-6936-457b-ff44-42eda71fd3d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (1): ReLU()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Flatten()\n",
       "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (8): ReLU()\n",
       "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet = create_lenet().to(device)\n",
    "lenet.load_state_dict(torch.load(\"lenet.pth\"))\n",
    "lenet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-v2Au488R8O"
   },
   "source": [
    "## Creating the Function to test validation data\n",
    "\n",
    "Although we were already using the validation data to find the accuracy of our model, we also want to see where the model got confused. In order to do this, we will have a list of predictions and ground truth for every image in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS7nBVc_8R8P"
   },
   "outputs": [],
   "source": [
    "def predict_dl(model, data):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for i, (images, labels) in enumerate(data):\n",
    "        images = images.cuda()\n",
    "        x = model(images)\n",
    "        value, pred = torch.max(x, 1)\n",
    "        pred = pred.data.cpu()\n",
    "        y_pred.extend(list(pred.numpy()))\n",
    "        y_true.extend(list(labels.numpy()))\n",
    "    return np.array(y_pred), np.array(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TD2wR798R8R"
   },
   "source": [
    "Call the function and store it in two numpy arrays that are returned from the *predict_dl* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqQ3-hPP8R8S"
   },
   "outputs": [],
   "source": [
    "y_pred, y_true = predict_dl(lenet, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqgPJYhv8R8U"
   },
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Now that we have two numpy arrays of the predictions and the ground truth, we can use the confusion matrix feature from sklearn.\n",
    "\n",
    "We are showing this confusion matrix through a pandas dataframe.\n",
    "\n",
    "Hopefully, since our model was somewhere in the neighborhood of 99% accurate with the validation dataset, there should be little confusion between digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9mC5YyY8R8V",
    "outputId": "2fff578c-af8a-4dd1-e925-93b1622eda69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>977</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>998</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>970</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>880</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>946</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1017</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>963</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3    4    5    6     7    8    9\n",
       "0  977     1     0    0    0    0    0     0    1    1\n",
       "1    0  1135     0    0    0    0    0     0    0    0\n",
       "2    2     0  1028    0    0    0    0     1    1    0\n",
       "3    0     0     3  998    0    5    0     1    3    0\n",
       "4    0     3     0    0  970    0    4     1    0    4\n",
       "5    3     0     0    5    0  880    2     1    1    0\n",
       "6    4     3     2    0    1    1  946     0    1    0\n",
       "7    1     3     1    0    1    0    0  1017    1    4\n",
       "8    2     0     2    1    1    1    0     2  963    2\n",
       "9    0     3     1    0    4    4    1     0    3  993"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_true, y_pred, labels=np.arange(0,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaR_csUk8R8Z"
   },
   "source": [
    "## Define Inference function to get prediction for any given image\n",
    "\n",
    "Now that we are confident that our model is pretty good at classifying digits, we can get any image with a single digit in it and see what the model predicts!\n",
    "\n",
    "We use the requests library to get the path and then we can access the contents of the path. This is where we use the *Image* module from *PIL* to open the image from the path. Then we need to resize it to (28, 28) in order to be accessible by the model. \n",
    "\n",
    "Finally, we call the model with the transformed image (from PIL image to tensor). We must send in an image as a float and set to the device we want to use. After the model predicts and returns an output, we need to do a *softmax* in order to normalize the output into probablity (0.0 to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VGEbYiZ8R8a"
   },
   "outputs": [],
   "source": [
    "def inference(path, model, device):\n",
    "    r = requests.get(path)\n",
    "    with BytesIO(r.content) as f:\n",
    "        img = Image.open(f).convert(mode=\"L\")\n",
    "        img = img.resize((28, 28))\n",
    "        x = (255 - np.expand_dims(np.array(img), -1))/255.\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.unsqueeze(T(x), axis=0).float().to(device))\n",
    "        return F.softmax(pred, dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jjw9NTnL8R8d"
   },
   "source": [
    "## Getting the image from the web\n",
    "\n",
    "We can simply just copy the image's address from the internet and paste it in the string *path*. Then, we can access the contents of the image and resize it, similar to what we did in the inference function.\n",
    "\n",
    "#### Important: If the image's address is really long, then there will be an error. If this happens, try using a different image with a smaller address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMZ-pqgo8R8d"
   },
   "outputs": [],
   "source": [
    "path = \"https://c8.alamy.com/comp/HXBRW0/2-red-handwritten-digits-over-white-background-HXBRW0.jpg\"\n",
    "r = requests.get(path)\n",
    "with BytesIO(r.content) as f:\n",
    "    img = Image.open(f).convert(mode=\"L\")\n",
    "    img = img.resize((28, 28))\n",
    "x = (255 - np.expand_dims(np.array(img), -1))/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JLmN29D8R8g"
   },
   "source": [
    "## Showing the image\n",
    "Now that we have successfully transformed the image and resized it, we can see what the image looks like. If the image looks very different (meaning that the image has been stretched or shrunk significantly) than what it originally was from the internet, chances are that the model might predict incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QykLYcrv8R8g",
    "outputId": "3f1cd429-f875-4738-9db5-c59ca52596ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f01f4d97f50>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.squeeze(-1), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yx-F0-u98R8j"
   },
   "source": [
    "## Predictions\n",
    "Now its time to get the prediction for the image. We can just call the *inference* function, which will then call the model itself. We have to tell the function which model to use, as well as which device to use: CPU or GPU.\n",
    "\n",
    "Once we receive the predictions, a numpy array of size 10 - for 0 to 9 digits, we have to find index of the array that has the largest value, which means that the model is most confident in that answer. To do this, we use the built-in *argmax* function of numpy.\n",
    "\n",
    "Finally, we can just print the index, which is the digit predicted, as well as the probability (as a percent done by multiplying by 100), which is nothing but the value of the *pred* array at the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YmPoZ3q8R8k",
    "outputId": "fd4a4f69-e8fc-4475-d5db-cfd5a96ef9dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 2, Prob: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "pred = inference(path, lenet, device=device)\n",
    "pred_idx = np.argmax(pred)\n",
    "print(f\"Predicted: {pred_idx}, Prob: {pred[0][pred_idx]*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMk90qJm8R8n"
   },
   "source": [
    "Here we can just show every the whole numpy array to see the model's prediction of the probability of each digit (0 - 9) being the digit in the image. A really good model should be able to distinguish greatly between the correct digit and the other 9 incorrect digits.\n",
    "\n",
    "In this case, only 2 is even close to or at 1 (100 % probability), whereas the other digits are very close to 0 (0 % probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CNJLq6f8R8o",
    "outputId": "670c65d2-b784-4b00-8c1d-07a8efa8caa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.8190421e-16, 4.1869504e-11, 1.0000000e+00, 8.2445465e-12,\n",
       "        2.7881241e-20, 5.0278743e-16, 5.4344697e-12, 1.3035397e-15,\n",
       "        3.0033712e-11, 1.5618705e-16]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51xmZSGu8R88"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Pws6gYLx8R7r",
    "iL5y5_5H8R7w",
    "4P7DUaLg8R70",
    "vOHWqdsE8R74",
    "F9xtlOiJ8R7_",
    "HdWTyLKL8R8I",
    "aKDnR9mm8R8L",
    "I-v2Au488R8O",
    "CqgPJYhv8R8U",
    "DaR_csUk8R8Z",
    "Jjw9NTnL8R8d",
    "0JLmN29D8R8g",
    "yx-F0-u98R8j"
   ],
   "name": "mnist_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
